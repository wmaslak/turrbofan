---
title: "Preprocessing"
output: html_document
---

```{r knit options, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE, results = "hold")
# loading packages
source("../../R/pkg_loader.R", local = knitr::knit_global())
```

# Preprocessing

## Importing the raw data and preprocessing


Let's import the raw datasets... 

```{r import raw , class.source = 'fold-show', cache=TRUE}

# putting dataframes' names on one list
# DATAS(imple) and DATAC(omplex)
data_train_names <- paste0("data",1:4)
data_test_names <- paste0("test_data",1:4)
data_rul_names <- paste0("rul",1:4)

data_names <- c(data_train_names,data_test_names,data_rul_names)
# putting names of desired files on one list
train_file_names <- c("train_FD001.txt","train_FD002.txt","train_FD003.txt","train_FD004.txt")
test_file_names <- c("test_FD001.txt","test_FD002.txt","test_FD003.txt","test_FD004.txt")
rul_file_names <- c("RUL_FD001.txt","RUL_FD002.txt","RUL_FD003.txt","RUL_FD004.txt")

file_names <- c(train_file_names,test_file_names,rul_file_names)

for (i in  seq_along( file_names ) ) {
  path <- paste0("../../raw_data/", file_names[i])
  assign(data_names[i],read.table(path, header = F))
}




```




...and set the headers according to the 
documentation.

```{r train test headers}

# creating column names
cols1 <- c("unit_no","time","op_set_1","op_set_2","op_set_3")
cols2 <- paste0("sensor_",c(1:21))
cols <- c(cols1,cols2)

# assigninng column names
for(df in c(data_train_names,data_test_names)) data.table::setnames(get(df),  cols)

```


## Verifying with metadata whether the data is correctly loaded 

Let's verify if the data is loaded correctly by comparing the number of engines and
number of operational settings to the [metadata](https://www.kaggle.com/behrad3d/nasa-cmaps). 

As operational settings values may fluctuate by small values we should look at numbers
rounded to the integer part. 

```{r verify and save}

# check number of engines
print("Number of unique engines in the dataset")
for (df in data_names) get(df)$unit_no %>% unique() %>% length() %>% paste(df,':',.) %>% print()

# check number of configuations
op_names <- paste("op_set",1:3)

print("Number of distinct combinations of operational settings levels (before rounding):")

for (df in c(data_train_names,data_test_names)) {
  get(df) %>%
    select(matches("op_set")) %>%
    distinct() %>%
    nrow() %>% paste(df, ':', .) %>% 
    print()
  }
  
print("Number of distinct combinations of operational settings levels (after rounding):")
for (df in c(data_train_names,data_test_names)){
  get(df) %>% 
    select(matches("op_set")) %>%
    round() %>% 
    distinct() %>% 
    nrow() %>% 
    paste(df,':',.) %>%
    print()
} 

```

## Calculating RUL


Now we may compute our target variable which for a given observation $r_j$ out of
a sequence of observations $r_1,r_2,...,r_{N_i}$ of unit $i$ will simply be
$$RUL(j)_{i} = N_i - j$$

In words, it is just a distance in number of cycles of a given row to
the last row recorded in the dataset for a given unit. Therefore, we implicitly 
assume that RUL decreases linearly, but we may later revisit this assumption if
neccesary. The chunk below calculates RUL for train sets.

 

```{r calculate RUL}

for(df in data_train_names){
  
# calculate RUL
 cp <- get(df) %>% 
  left_join(. ,get(df) %>% 
              group_by(unit_no) %>% 
              summarise(max_time = max(time)), by = "unit_no" ) %>% 
  mutate(RUL = max_time - time, max_time = NULL) 
 
# assign back with the new column added
 assign(df,cp)
 rm(cp)
 
}
```

## Adding operational settings column

For further convenience it is good to add a column to distinguish between
the six different operational settings in the complex dataset. Luckily, we can
find a simple 1-1 mapping between the values of operational settings and their types,
namely sum of the columns. 

```{r}

for (i in seq(2,8,2)) {
  
  
 tmp <-  get(data_names[i]) %>% 
  mutate(across(.cols=matches("op_set"), round)) %>% 
  mutate(setting = rowSums(across(.cols=matches("op_set"))))
 
 assign(data_names[i],tmp)
  
}

```


Let's make sure that the sum can actually identify each setting:


```{r}
data4 %>% 
  mutate(across(.cols=matches("op_set"), round)) %>% 
  mutate(setting = rowSums(across(.cols=matches("op_set")))) %>% 
  group_by(op_set_1,op_set_2,op_set_3,setting) %>% summarise(n = n())
```

## Tidying up test sets and saving preprocessed files as R objects

Let's also tidy the test sets by adding the RUL from the separate files.
If we decide to use lagged predictors we can always use it to create corresponding
variables here.

Additionally we will create a simplified versions of the test datasets.
They will contain only the last row for each unit and the associated RUL.
This will be handy, as our task is to predict the RUL for the last available point. 


```{r  tidy test sets}

# adding RUL
for (i in seq_along(data_test_names) ) {
  
  cp <- get(data_rul_names[i]) %>% `colnames<-`("act_RUL")
  cp <- get(data_test_names[i]) %>%
    left_join( . ,
              cp %>% 
                mutate(unit_no = 1:nrow(cp)) ,
              by="unit_no")
  
  assign(data_test_names[i],cp)
  rm(cp)
}

# creating the simplified test sets
for (df in data_test_names ) {
  
  cp <- get(df) %>% 
    group_by(unit_no) %>% 
    filter(row_number()==n())
  
  assign(paste0(df,"_s"),cp)
  rm(cp)
}

```


Now we are ready to save the preprocessed datasets as R objects

```{r save data}

data_simplified_test_names <- paste0(data_test_names, "_s") 
data_names <- c(data_names,data_simplified_test_names)

for (df in data_names) {
  obj_name <- paste0(df,'.Rds')
  subfolder <- case_when(grepl(".*test.*",df) ~ "test_data",
                         grepl(".*rul.*",df) ~ "rul_data",
                         !grepl("(test)|(rul)",df) ~ "train_data")
                         

 get(df) %>% saveRDS(file = paste0('../../data/',subfolder,'/',obj_name))
}

```





